# streamlit run app10.py
# Version review:
# - Open QA chat: LearnLM
# - True/False questions: o4-mini (only generation) and LearnLM (as help chat)

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
import os
#from dotenv import load_dotenv
from openai import OpenAI
from openai.types.responses.response_output_message import ResponseOutputMessage
from google import genai
import io
from fpdf import FPDF
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import time
from google.genai import types
import re
import pickle
from datetime import datetime



# Load environment variables from a .env file
#load_dotenv()
OPENAI_API = st.secrets.get("OPENAI_API_KEY")
GOOGLE_API = st.secrets.get("GOOGLE_API_KEY")
#OPENAI_API = os.getenv('OPENAI_API_KEY')
#GOOGLE_API = os.getenv('GOOGLE_API_KEY')
embedding = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=OPENAI_API)
db_openai = Chroma(persist_directory="./vectordb/openai_vectorDB/", embedding_function=embedding) #for existing database
MODEL_QUESTION_GENERATION = "o4-mini"  # Model for question generation
#MODEL_QUESTION_GENERATION = "gpt-3.5-turbo"  # Model for question generation


def get_page_numbers(page_numbers: list[str]) -> list[str]:
    """
    Get the page numbers from the metadata.

    Args:
        page_numbers (list): list of page numbers

    Returns:
        list: sorted list of page numbers
    """
    int_page_numbers = []
    for page_number in page_numbers:
        if "and" in page_number:
            page_number = page_number.split(" and ")
            page_number = [int(i) for i in page_number]
            int_page_numbers.extend(page_number)
        else:
            page_number = int(page_number)
            int_page_numbers.append(page_number)

    # Unique the page numbers
    int_page_numbers = list(set(int_page_numbers))

    # Sort the page numbers
    int_page_numbers.sort()

    # Now we need to string them again
    return [str(i) for i in int_page_numbers]


def intro():
    st.write("# Virtual Teaching Assistant for Linear Algebra.👋 :mortar_board: :computer: :books: :school:")
    st.write("## :warning: THIS TOOL IS UNDER DEVELOPMENT :construction_worker:")
    #st.sidebar.success("Select a practice mode.")

    st.markdown(
        """
        This app is in demo mode for DACS students at Maastricht University, the Netherlands.
        Please go to the DEMO page using the sidebar navigation.

        Survey link: [Survey](https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_3f9d79itNZlNvFQ)
    """
    )
    #    This app is a Virtual Teaching Assistant for Linear Algebra.
    #    It is designed to help students learn Linear Algebra concepts, like:
    #    - solving linear systems of equations;
    #    - matrix algebra;
    #    - determinants;
    #    - eigenvalues and eigenvectors;
    #    through the use of a chat interface and true/false questions.
    #"""
    #)

def stream_string_data(text: str):
    for word in text.split(" "):
        yield word + " "
        time.sleep(0.02)

def chat_TF_generation(help_TF_Q: dict, idx: int):
    st.markdown("## Help chat")
    st.markdown("This chat interface allows you to ask questions about the True/False statement generated by the system. Please note that this window will dissapear when you submit your answer to the question.")
    
    genai_client = genai.Client(api_key=GOOGLE_API)

    #if "openai_model" not in st.session_state:
    #    #st.session_state["openai_model"] = "gpt-3.5-turbo"
    #    st.session_state["openai_model"] = "o4-mini"
    if "gemini_model" not in st.session_state:
        st.session_state["gemini_model"] = "learnlm-2.0-flash-experimental"

    if f"messages_{idx}" not in st.session_state:
        st.session_state[f"messages_{idx}"] = []

    if f"init_chat_TF_{idx}" not in st.session_state:
        st.session_state[f"init_chat_TF_{idx}"] = True

    for message in st.session_state[f"messages_{idx}"]:
        if message["role"] == "references":
            with st.chat_message("references", avatar="📖"):
                st.markdown(message["content"])
        else:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


    question = help_TF_Q.get("question", "")
    explanation = help_TF_Q.get("explanation", "")
    answer = help_TF_Q.get("answer", "")
    #query = None

    if question=="" or explanation=="" or answer=="":
        st.error("Error: The question, explanation, or answer is missing.")
    elif st.session_state[f"init_chat_TF_{idx}"] == True:
        st.session_state[f"init_chat_TF_{idx}"] = False
        print("Starting")
        # Build custom prompt
        custom_prompt = (
            "You are an assistant for question-answering tasks in linear algebra. "
            "The user would like to get clarification on a True/False question. "
            "The question is: " + question + ". "
            "The explanation is: " + explanation + ". "
            "The answer is: " + str(answer) + ". "
            "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
            "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
            "You can write and run code to answer the question. "
            "If you don't know the answer, say that you don't know. "
            "Do not answer the question directly, but rather ask for clarification or additional information if needed. "
        )

        chat = genai_client.chats.create(
            model=st.session_state["gemini_model"],
            config=types.GenerateContentConfig(system_instruction=custom_prompt, tools=[types.Tool(code_execution=types.ToolCodeExecution)]),                
        )

        st.session_state[f"chat_{idx}"] = chat
        
    if query := st.chat_input("How can I help you with this True/False question?"):
        st.session_state[f"messages_{idx}"].append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)
        
        # Retrieve relevant context from the vector database
        query_rag = f"Question: {question}\nExplanation: {explanation}\nAnswer: {answer}" 
        retrieved_docs = db_openai.similarity_search(query_rag, k=4)
        context = "\n\n".join(doc.page_content for doc in retrieved_docs)
        references = get_page_numbers([doc.metadata['page'] for doc in retrieved_docs])
        print(references)

        total_query = f"Use the following pieces of retrieved context to aid the explanation: {context}\n\nUser query: {query}"

        try:
            response = st.session_state[f"chat_{idx}"].send_message(total_query)
            answer = response.text
        except Exception as e:
            st.error(f"Error processing chat clarification question.\nError: {e}")

        answer = transform_latex_text(str(answer))

        with st.chat_message("assistant"):
            answer = st.write_stream(stream_string_data(str(answer)))
        st.session_state[f"messages_{idx}"].append({"role": "assistant", "content": answer})

        if references:
            with st.chat_message("references", avatar="📖"):
                if len(references) == 1:
                    ref = f"Reference: page {references[0]}"
                else:
                    ref = f"References: pages {", ".join(references[:-1])}, and {references[-1]}"
                st.write(ref)
            #st.session_state.messages.append({"role": "assistant", "content": response})
            st.session_state[f"messages_{idx}"].append({"role": "references", "content": ref})


def chat_demo(question: str, o4_llm_state: dict):
    st.markdown("## Help chat")
    st.markdown("This chat interface allows you to ask questions about the True/False statement generated by the system. Please note that this window will dissapear when you submit your answer to the question.")
    st.markdown("If you click on 'True' or 'False', the answer will be displayed and the chat will be closed.")
    st.markdown("If you click on 'Force return', the chat will be closed and you will be returned to the question selector menu. There is delay of 5s.")

    #openai_client = OpenAI(api_key=OPENAI_API)
    genai_client = genai.Client(api_key=GOOGLE_API)

    if "openai_model" not in st.session_state:
        st.session_state["openai_model"] = "o4-mini"
    if "gemini_model" not in st.session_state:
        st.session_state["gemini_model"] = "learnlm-2.0-flash-experimental"

    if "messages_demo" not in st.session_state:
        st.session_state["messages_demo"] = []

    if "init_chat_TF_demo" not in st.session_state:
        st.session_state["init_chat_TF_demo"] = True

    for message in st.session_state["messages_demo"]:
        if message["role"] == "references":
            with st.chat_message("references", avatar="📖"):
                st.markdown(message["content"])
        else:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


    #question = help_TF_Q.get("question", "")
    #explanation = help_TF_Q.get("explanation", "")
    #answer = help_TF_Q.get("answer", "")
    #query = None

    if question=="":
        st.error("Error: The question is missing.")
    elif st.session_state["init_chat_TF_demo"] == True:
        st.session_state["init_chat_TF_demo"] = False
        print("Starting")

        explanation = o4_llm_state.get("explanation", "")
        answer = o4_llm_state.get("answer", "")

        if explanation == "" or answer == "":
            print(f"Explanation: {explanation}")
            print(f"Answer: {answer}")
            st.error("Error: The explanation or answer is missing from the response.")
        #if len(questions) != len(answers) or len(questions) != st.session_state.num_questions_final:	
        #    st.error("Error: The number of questions and answers do not match the requested number.")
        time.sleep(1) # Simulate processing time
        # Store generated questions and answers in session state
        #st.session_state.generated_questions = [
        #    {"question": q, "explanation": e, "answer": a} for q, e, a in zip(questions, explanations, answers)
        #]

        #if "explanation_demo" not in st.session_state:
            # Store the full explanation, preserving line breaks and LaTeX
            #st.session_state["explanation_demo"] = explanation.strip().replace('\n', ' ')

        # Build custom prompt
        custom_prompt_gemini = (
            "You are an assistant for question-answering tasks in linear algebra. "
            "The linear algebra course is taught at a university level (Bachelor). "
            "The user would like to get clarification on a True/False question. "
            "The question is: " + question + ". "
            "The explanation is: " + explanation + ". "
            "The answer is: " + str(answer) + ". "
            "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
            "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
            "You can write and run code to answer the question. "
            "DO NOT GIVE THE ANSWER TO THE QUESTION DIRECTLY. "
            "Do not give the answer to the student. "
            "DO NOT REFER TO THE EXPLANATION GIVEN TO YOU AS THE STUDENT DOES NOT HAVE ACCESS TO IT. USE IT TO HELP THE STUDENT. "
            "The students should learn from iteracting. "
            "You can give hints, but also ask for clarification or additional information if needed. "
        )

        chat = genai_client.chats.create(
            model=st.session_state["gemini_model"],
            config=types.GenerateContentConfig(system_instruction=custom_prompt_gemini, tools=[types.Tool(code_execution=types.ToolCodeExecution)]),                
        )

        st.session_state["chat_demo"] = chat
        
    if query := st.chat_input("How can I help you with this True/False question?"):
        if query.lower() == "return":
            st.session_state["init_chat_TF_demo"] = True
            st.session_state["messages_demo"] = []
            st.session_state["chat_demo"] = None
            st.session_state.return_chat_TF_demo = True
            st.rerun()
        st.session_state["messages_demo"].append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)
        
        # Retrieve relevant context from the vector database
        #query_rag = f"Question: {question}\nExplanation: {explanation}\nAnswer: {answer}" 
        retrieved_docs = db_openai.similarity_search(query, k=4)
        context = "\n\n".join(doc.page_content for doc in retrieved_docs)
        references = get_page_numbers([doc.metadata['page'] for doc in retrieved_docs])
        print(references)

        total_query = f"Use the following pieces of retrieved context to aid the explanation: {context}\n\nUser query: {query}"

        try:
            response = st.session_state["chat_demo"].send_message(total_query)
            answer = response.text
        except Exception as e:
            st.error(f"Error processing chat clarification question.\nError: {e}")

        answer = transform_latex_text(str(answer))

        with st.chat_message("assistant"):
            answer = st.write_stream(stream_string_data(str(answer)))
        st.session_state["messages_demo"].append({"role": "assistant", "content": answer})

        if references:
            with st.chat_message("references", avatar="📖"):
                if len(references) == 1:
                    ref = f"Reference: page {references[0]}"
                else:
                    ref = f"References: pages {", ".join(references[:-1])}, and {references[-1]}"
                st.write(ref)
            #st.session_state.messages.append({"role": "assistant", "content": response})
            st.session_state["messages_demo"].append({"role": "references", "content": ref})

def chat():
    st.title("QA chat for Linear Algebra")
    st.markdown("This chat interface allows you to ask open questions about Linear Algebra. Type 'clear' to reset the chat history.")

    genai_client = genai.Client(api_key=GOOGLE_API)

    if "gemini_model" not in st.session_state:
        st.session_state["gemini_model"] = "learnlm-2.0-flash-experimental"

    if "messages_chat" not in st.session_state:
        st.session_state.messages_chat = []

    for message in st.session_state.messages_chat:
        if message["role"] == "references":
            with st.chat_message("references", avatar="📖"):
                st.markdown(message["content"])
        else:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    if query := st.chat_input("Ask a question about Linear Algebra."):
        if "clear" == query.lower():  
            #print(st.session_state.messages_chat)
            safe_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            with open(f'chats/Open_chat_A_list_{safe_time}.pkl', 'wb') as f:
                pickle.dump(st.session_state.messages_chat, f)
            st.session_state.messages_chat = []
            if "open_chat" in st.session_state:
                del st.session_state["open_chat"]
            st.rerun()
        else:
            st.session_state.messages_chat.append({"role": "user", "content": query})
            with st.chat_message("user"):
                st.markdown(query)
            #print(query)

            # Retrieve relevant context from the vector database
            retrieved_docs = db_openai.similarity_search(query, k=4)
            context = "\n\n".join(doc.page_content for doc in retrieved_docs)
            references = get_page_numbers([doc.metadata['page'] for doc in retrieved_docs])
            print(references)

            # Build custom prompt
            # Combine message history with the custom prompt as system message
            custom_prompt = (
                "You are an assistant for question-answering tasks in linear algebra. "
                "If the question is not related to linear algebra, politely decline to answer it and put at the end of the repsonse REFUSED=TRUE. "
                "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
                "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
                "If you don't know the answer, say 'I don't know'. "
                "You can write and run code to answer the question. "
                "Use the following pieces of retrieved context to answer the question. "
                "\n\nContext:\n" + context
            )


            # Prepare messages: system prompt + previous messages + latest user query
            messages = [{"role": "system", "content": custom_prompt}]
            messages += [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages_chat if m["role"] != "references"]
            # Overwrite the last user message to ensure it includes the context
            if messages and messages[-1]["role"] == "user":
                messages[-1]["content"] = query

            # Use the Google Gemini API to generate a response
            if "open_chat" not in st.session_state:
                chat = genai_client.chats.create(
                    model=st.session_state["gemini_model"],
                    config=types.GenerateContentConfig(system_instruction=custom_prompt, tools=[types.Tool(code_execution=types.ToolCodeExecution)]),                
                )
                st.session_state["open_chat"] = chat
            
            try:
                response = st.session_state["open_chat"].send_message(query)
                response_LearnLM = response.text
            except Exception as e:
                st.error(f"Error processing chat clarification question.\nError: {e}")
            response = transform_latex_text(str(response_LearnLM))

            # Check if the response contains REFUSED=TRUE
            if "REFUSED=TRUE" in response:
                response = response.replace("REFUSED=TRUE", "")
                references = None  # No references to show if refused
            else:
                if "refused=false" in response.lower():
                    response = response.replace("REFUSED=FALSE", "")
                    response = response.replace("refused=false", "")
                if "refused = false" in response.lower():
                    response = response.replace("REFUSED = FALSE", "")
                    response = response.replace("refused = false", "") 
            with st.chat_message("assistant"):
                response_LearnLM = st.write_stream(stream_string_data(response))
            st.session_state.messages_chat.append({"role": "assistant", "content": response_LearnLM})
            if references:
                with st.chat_message("references", avatar="📖"):
                    if len(references) == 1:
                        ref = f"Reference: page {references[0]}"
                    else:
                        ref = f"References: pages {", ".join(references[:-1])}, and {references[-1]}"
                    st.write(ref)
                st.session_state.messages_chat.append({"role": "references", "content": ref})
            #print(st.session_state.messages_chat)

def transform_latex_text(text):
    # Replace display math block with inline math (square brackets to dollar signs)
    text = re.sub(r"\[\s*(.*?)\s*\]", r'$\1$', text, flags=re.DOTALL)

    # Replace LaTeX alignment characters
    text = text.replace(r"\[6pt]", r"\\\\")  # Replace spacing with newline
    text = text.replace(";", "")  # Remove spacing semicolons

    # Ensure escaped backslashes are doubled (for inline math inside string)
    #text = text.replace(r'\begin{pmatrix}', r'\\begin{pmatrix}')
    #text = text.replace(r'\end{pmatrix}', r'\\end{pmatrix}')
    #text = text.replace(r'\\', r'\\\\')

    return text

def transform_latex_text2(text):
    # Replace display math block with inline math (square brackets to dollar signs)
    text = re.sub(r"\(\s*(.*?)\s*\)", r'$\1$', text, flags=re.DOTALL)

    # Replace LaTeX alignment characters
    text = text.replace(r"\[6pt]", r"\\\\")  # Replace spacing with newline
    text = text.replace(";", "")  # Remove spacing semicolons

    # Ensure escaped backslashes are doubled (for inline math inside string)
    #text = text.replace(r'\begin{pmatrix}', r'\\begin{pmatrix}')
    #text = text.replace(r'\end{pmatrix}', r'\\end{pmatrix}')
    #text = text.replace(r'\\', r'\\\\')

    return text

def generate_questions():
    
    # List of True/False questions with answers
    questions = [
        {
            "question": "If a square matrix A is invertible, then its determinant is zero.",
            "answer": False
        },
        {
            "question": "The transpose of a product of two matrices equals the product of their transposes in reverse order.",
            "answer": True
        },
        {
            "question": "All eigenvalues of an orthogonal matrix have absolute value 1.",
            "answer": True
        }
    ]
    return questions


def extract_llm_response_code_interpreter(response: list) -> str:
    output = ""
    #print(response)
    #print(len(response))
    for index in range(len(response)-1, -1, -1):
        if isinstance(response[index], ResponseOutputMessage):
            #print(len(response[out].content)) check if this length is 1
            #print(f"Output {index}:\n{response[index].content[0].text}")
            output = response[index].content[0].text
            break #stop after the answer is found
    return output

def practice_true_false_questions():
    questions = generate_questions()
    num_questions = len(questions)

    # === Session State Initialization ===
    if "question_index" not in st.session_state:
        st.session_state.question_index = 0
    if "submitted" not in st.session_state:
        st.session_state.submitted = [False] * num_questions
    if "user_answers" not in st.session_state:
        st.session_state.user_answers = [None] * num_questions
    if "show_score" not in st.session_state:
        st.session_state.show_score = False
    if "review_md" not in st.session_state:
        st.session_state.review_md = None

    def calculate_score():
        return sum(
            1 for i, q in enumerate(st.session_state.generated_questions)
            if st.session_state.user_answers[i] == q["answer"]
        )

    def generate_review_markdown():
        lines = ["# Quiz Review\n"]
        score = calculate_score()
        lines.append(f"## Final Score: {score} / {num_questions}")
        for i, q in enumerate(questions):
            user_ans = st.session_state.user_answers[i]
            correct = q["answer"]
            result = "✅ Correct" if user_ans == correct else "❌ Incorrect"
            lines.append(f"### Question {i+1}: {q['question']}")
            #lines.append(f"**Q:** {q['question']}")
            lines.append(f"- Your answer: **{user_ans}**")
            lines.append(f"- Correct answer: **{correct}**")
            lines.append(f"- Result: {result}\n")

        return "\n".join(lines)


    def generate_review_pdf():
        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", size=12)

        score = calculate_score()
        pdf.set_font("Arial", 'B', 14)
        pdf.cell(0, 10, "Quiz Review", ln=True)
        pdf.set_font("Arial", size=12)
        pdf.cell(0, 10, f"Final Score: {score} / {st.session_state.num_questions_final}", ln=True)
        #for i, q in enumerate()
        for i, q in enumerate(st.session_state.generated_questions):
            user_ans = st.session_state.user_answers[i]
            result = "Correct" if user_ans == q["answer"] else "Incorrect"
            pdf.ln(5)
            pdf.set_font("Arial", 'B', 12)
            pdf.multi_cell(0, 10, f"Question {i+1}: {q['question']}")
            pdf.set_font("Arial", size=12)
            pdf.cell(0, 10, f"- Your answer: {user_ans}", ln=True)
            pdf.cell(0, 10, f"- Correct answer: {q["answer"]}", ln=True)
            pdf.multi_cell(0, 10, f"- Explanation: {q['explanation']}")
            pdf.cell(0, 10, f"- Result: {result}", ln=True)

        # Get PDF as bytes
        pdf_str = str(pdf.output(dest='S'))
        pdf_bytes = pdf_str.encode('latin1')
        return io.BytesIO(pdf_bytes)

    # === Main UI ===
    st.write("# Practise True/False Questions")
    # Topic selection with radio buttons (max 5 active)
    topics = [
        "Systems of linear equations, Gaussian elimination",
        "Vector equations, Matrix",
        "Solution sets and Linear independence",
        "Linear Transformations, Matrix algebra",
        "The Inverse of a Matrix",
        "Determinants, Perspective projections",
        "Vector Spaces",
        "Eigenvalues and Eigenvectors",
        "Diagonalization",
        "Orthogonality and Symmetric Matrices"
    ]

    # Topic selection UI
    if "topics_submitted" not in st.session_state:
        st.session_state.topics_submitted = False

    if "generated_questions" not in st.session_state:
        st.session_state.generated_questions = None

    if not st.session_state.topics_submitted:
        st.write("The system will generate True/False questions about Linear Algebra. You can now choose which topics you want to practice.")
        selected_topics = st.multiselect(
            "Select up to 5 topics to practice:",
            topics,
            max_selections=5
        )
        if len(selected_topics) == 5:
            st.info("Maximum of 5 topics selected.")
        st.session_state.selected_topics = selected_topics
        
        # Slider for number of questions
        num_questions_slider = st.slider(
            "Select the number of questions to practice:",
            min_value=1,
            max_value=10,
            value=len(selected_topics),
            step=1
        )

        # Button to generate questions
        if st.button("Generate Questions"):
            if not selected_topics:
                st.warning("Please select at least one topic.")
            else:
                st.session_state.topics_submitted = True
                st.session_state.selected_topics_final = selected_topics
                st.session_state.num_questions_final = num_questions_slider
        st.stop()
    elif st.session_state.generated_questions is None:
        # Hide topic selection and slider after questions are generated
        with st.spinner("Generating questions, please wait..."):
            num_questions = str(st.session_state.num_questions_final)
            topics_str = ", ".join(st.session_state.selected_topics_final)
            custom_prompt = (
                "You are a tutor for generating True/False statements in Linear Algebra. "
                "Generate exactly " + num_questions + " True/False questions based on the selected topics. "
                "Please format as follows:\n"
                "- Each question should start with 'Q:' followed by the question text.\n"
                "- Each explanation should start with 'E:' followed by the explanation text.\n"
                "- Each answer should start with 'A:' followed by 'True' or 'False'.\n"
                "For example:\n"
                "Q: The determinant of a matrix is always non-negative.\n"
                "E: The determinant can be negative depending on the matrix. For example, $\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$.\n"
                "A: False\n"
                "Please use LaTeX formatting for mathematical expressions by writing them between dollar signs."
                "For example, to write a matrix, use $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. "
                "You can write and run code to answer the question. "
                "\n\nSelected topics:\n" + topics_str
            )
            llm = OpenAI(api_key=OPENAI_API)

            if "openai_model" not in st.session_state:
                st.session_state["openai_model"] = MODEL_QUESTION_GENERATION
            if "container_id" not in st.session_state:
                print("Creating container for Code Interpreter...")
                cont = llm.containers.create(name="test")
                container_id_manual = cont.id
                st.session_state["container_id"] = container_id_manual

            try:
                response = llm.responses.create(
                    model=st.session_state["openai_model"],
                    tools=[{"type": "code_interpreter", "container": st.session_state["container_id"]}],
                    input=custom_prompt,
                )
                # Extract the answer from the response
                answer = extract_llm_response_code_interpreter(response.output)
                if answer == "":
                    raise ValueError("Empty answer received from LLM.")
                

            except Exception as e:
                print(f"Error generating questions.\nError: {e}")
                st.error(f"Error generating questions.\nError: {e}")

            # Use the correct message format for OpenAI
            #response = llm.chat.completions.create(
            #    model=st.session_state["openai_model"],
            #    messages= [{"role": "system","content": custom_prompt}],
            #    stream=False,
            #)
            #response_content = response.choices[0].message.content
            # Process the response to extract questions and answers
            response_content = answer
            questions = []
            explanations = []
            answers = []
            for line in response_content.split("\n"):
                line = line.strip()
                if line.startswith("Q:"):
                    question = line[2:].strip()
                    questions.append(question)
                    #questions.append(transform_latex_text2(question))
                elif line.startswith("E:"):
                    explanation = line[2:].strip()
                    explanations.append(explanation)
                    #explanations.append(transform_latex_text2(explanation))
                elif line.startswith("A:"):
                    answer = line[2:].strip().lower() == "true"
                    answers.append(answer)
            if len(questions) != len(answers) or len(questions) != st.session_state.num_questions_final:	
                st.error("Error: The number of questions and answers do not match the requested number.")
            time.sleep(1) # Simulate processing time
            # Store generated questions and answers in session state
            st.session_state.generated_questions = [
                {"question": q, "explanation": e, "answer": a} for q, e, a in zip(questions, explanations, answers)
            ]
            print(st.session_state.generated_questions)
            st.session_state.question_index = 0
            st.session_state.submitted = [False] * len(questions)
            st.session_state.user_answers = [None] * len(questions)
            st.session_state.show_score = False
            st.session_state.review_md = None

    # Display all questions with separate True/False buttons
    if st.session_state.generated_questions is not None:
        st.success("Questions generated!")
        st.markdown(
            f"**Selected topics:** {', '.join(st.session_state.selected_topics_final)}  \n"
            f"**Number of questions:** {st.session_state.num_questions_final}"
        )
        st.write("### Answer the following questions:")
        for idx, question_data in enumerate(st.session_state.generated_questions):
            #st.markdown(f"**Question {idx + 1}:** {question_data['question']}")
            st.write(f"**Question {idx + 1}:** {question_data['question']}")
            col1, col2, col3 = st.columns([1, 1, 1])
            help_key = f"help_active_{idx}"
            with col1:
                if st.button(f"True", key=f"true_btn_{idx}", disabled=st.session_state.submitted[idx]):
                    user_answers = st.session_state.user_answers
                    user_answers[idx] = True
                    st.session_state.user_answers = user_answers
                    submitted = st.session_state.submitted
                    submitted[idx] = True
                    st.session_state.submitted = submitted
                    # Reset help state for this question
                    st.session_state[help_key] = False
            with col2:
                if st.button(f"False", key=f"false_btn_{idx}", disabled=st.session_state.submitted[idx]):
                    user_answers = st.session_state.user_answers
                    user_answers[idx] = False
                    st.session_state.user_answers = user_answers
                    submitted = st.session_state.submitted
                    submitted[idx] = True
                    st.session_state.submitted = submitted
                    # Reset help state for this question
                    st.session_state[help_key] = False
            # Use a session state key to track which help button was pressed
            with col3:
                if st.button("Help", key=f"help_btn_{idx}"):
                    # Set the help key for this question to True, others to False
                    for i in range(len(st.session_state.generated_questions)):
                        st.session_state[f"help_active_{i}"] = (i == idx)
            # Show chat across all columns if help is active for this question
            if st.session_state.get(help_key, False) and not st.session_state.submitted[idx]:
                #st.markdown("---")
                chat_TF_generation(question_data, idx=idx)
                st.markdown("---")
            # Show feedback if answered
            if st.session_state.submitted[idx]:
                correct_answer = question_data['answer']
                explanation = question_data['explanation']
                user_answer = st.session_state.user_answers[idx]
                if user_answer == correct_answer:
                    st.success(f"Question {idx + 1}: Correct! 🎉\n\nExplanation: {explanation}")
                else:
                    st.error(f"Question {idx + 1}: Incorrect. The correct answer is {'True' if correct_answer else 'False'}.\n\nExplanation: {explanation}")

        # Optionally, show score or review after all questions are answered
        if all(st.session_state.submitted):
            score = sum(
                1 for i, q in enumerate(st.session_state.generated_questions)
                if st.session_state.user_answers[i] == q["answer"]
            )
            st.info(f"Quiz complete! Your score: {score} / {len(st.session_state.generated_questions)}")

            ## PDF option
            pdf_data = generate_review_pdf()
            st.download_button(
                label="Download Review as PDF",
                data=pdf_data,
                file_name="quiz_review.pdf",
                mime="application/pdf"
            )
            if st.button("Reset and Start Again"):
                for key in [
                    "topics_submitted",
                    "selected_topics",
                    "selected_topics_final",
                    "num_questions_final",
                    #"generated_questions",
                    "question_index",
                    "submitted",
                    "user_answers",
                    "show_score",
                    "review_md",
                ]:
                    if key in st.session_state:
                        del st.session_state[key]

                for idx, question_data in enumerate(st.session_state.generated_questions):
                    if f"messages_{idx}" in st.session_state:
                        safe_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
                        with open(f'chats/TF_chat_{idx}_list_{safe_time}.pkl', 'wb') as f:
                            pickle.dump(st.session_state[f"messages_{idx}"], f)
                        del st.session_state[f"messages_{idx}"]
                    if f"init_chat_TF_{idx}" in st.session_state:
                        del st.session_state[f"init_chat_TF_{idx}"]
                    if f"chat_{idx}" in st.session_state:
                        del st.session_state[f"chat_{idx}"]
                if "generated_questions" in st.session_state:
                    del st.session_state["generated_questions"]
                st.rerun()


def extract_question_answer(tex_content):
    # Extract content within the enumerate environment
    enum_match = re.search(r'\\begin{enumerate}(.*?)\\end{enumerate}', tex_content, re.DOTALL)
    if not enum_match:
        return [], []
    enum_content = enum_match.group(1)

    # Find all questions (\item ... \begin{solutionorbox})
    question_blocks = re.findall(
        r'\\item(.*?)(?=\\begin{solutionorbox})',
        enum_content, re.DOTALL
    )

    # Find all answers (\begin{solutionorbox} ... \end{solutionorbox})
    answer_blocks = re.findall(
        r'\\begin{solutionorbox}\[[^\]]*\]\s*(.*?)\\end{solutionorbox}',
        enum_content, re.DOTALL
    )
    questions = [q.strip() for q in question_blocks]
    answers = [a.strip() for a in answer_blocks]
    return questions, answers

def read_questions_answers():
    # Split out True/False questions
    exam_questions_TF = []
    exam_answers_TF = []

    with open('exams/together2.tex', 'r', encoding='utf-8') as file:
        tex_content = file.read()
        questions, answers = extract_question_answer(tex_content)
        #print(f"Total Questions: {len(questions)}")
        #print(f"Total Answers: {len(answers)}")
        if len(questions) != len(answers):
            print("Warning: The number of questions and answers do not match!")
 
        #print()
        for idx, (q, a) in enumerate(zip(questions, answers), 1):
            #print(f"Question {idx}:\n{q}\n")
            #print(f"Answer {idx}:\n{a}\n")
            exam_questions_TF.append(q)
            exam_answers_TF.append(a)
    return exam_questions_TF, exam_answers_TF

def save_conversation(correct_answer_bool: bool| None):
    data = {
        "correct_answer": correct_answer_bool,
        "timestamp_begin": st.session_state["begin_timestamp"],
        "timestamp_saved": datetime.now(),
        "selected_question": st.session_state["selected_question"],
        "messages": st.session_state["messages_demo"]
    }
    file = pickle.dumps(data)

    return file

def demo():
    st.write("# DEMO: Linear Algebra Virtual Teaching Assistant")
    st.markdown("This is a demo of the Virtual Teaching Assistant for Linear Algebra. You can try out the chat interface and practice True/False questions.")
    questions, answers = read_questions_answers()
    if "question_submitted" not in st.session_state:
        st.session_state.question_submitted = False

    if "o4_llm_state" not in st.session_state:
        o4_llm_state = pickle.load(open('exams/together2_llm_state.pkl', 'rb'))

    if not st.session_state.question_submitted:
        st.markdown("Survey link: [Survey](https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_3f9d79itNZlNvFQ)")
        question_options = [f"Q{i}" for i in range(1, 9)]
        selected_question = st.selectbox("Select a question to view:", question_options)
        st.session_state.selected_question = selected_question # Always update to current selection
        st.write(f"You selected: {selected_question}")
        #print(st.session_state.selected_question[1:])

        if st.button("Confirm and continue"):
            st.success(f"Proceeding to the next page with {selected_question}...")
            st.session_state.question_submitted = True
            st.rerun()
    elif st.session_state.question_submitted:
        st.success("You have selected a question. You can now proceed to the chat with the model to get hints to the answer of the True/False questions.")
        st.session_state["begin_timestamp"] = datetime.now()
        selected_question_number = int(st.session_state.selected_question[1:]) - 1  # Convert Q1 to index 0
        question = questions[selected_question_number]
        answer = answers[selected_question_number]

        o4_state = {
            "explanation": o4_llm_state["explanations"][selected_question_number],
            "answer": o4_llm_state["answers"][selected_question_number]
        }

        # Remove LaTeX bold formatting
        question = question.replace("\\textbf{always}", "*always*")  
        question = question.replace("\\textbf{Every}", "*Every*")
        question = question.replace("\\textbf{any}", "*any*")
        question = question.replace("\\textbf{rotation}", "*rotation*")
        question = question.replace("\\textbf{distinct}", "*distinct*")

        answer = answer.replace("\\textbf{True}", "*True*")
        answer = answer.replace("\\textbf{False}", "*False*")
        answer = answer.replace("\\textbf{not}", "*not*")

        st.write(f"### Question:")
        st.write(f"#### {question}")
        st.warning(f"Please do not click the True/False buttons yet, but try chat with the model to get towards the answer through asking concepts.", icon="⚠️")
        col1, col2, col3 = st.columns(3)
        if col1.button("True", key="demo_true_btn", disabled=st.session_state.get("demo_answer_submitted", False)):
            st.session_state.demo_answer_submitted = True
            st.session_state.demo_user_answer = True
        if col2.button("False", key="demo_false_btn", disabled=st.session_state.get("demo_answer_submitted", False)):
            st.session_state.demo_answer_submitted = True
            st.session_state.demo_user_answer = False
        if col3.button("Force return", key="return_btn", disabled=st.session_state.get("demo_answer_submitted", False)):
            file = save_conversation(None)
            st.download_button(
                label="Download conversations",
                data=file,
                file_name=f"data {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}.pkl",
                #mime="application/pdf"
            )
            time.sleep(5)
            for key in [
                "selected_question",
                "question_submitted",
                "demo_answer_submitted",
                "init_chat_TF_demo",
                "messages_demo",
                "chat_demo"
            ]:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()


        if not st.session_state.get("demo_answer_submitted", False):
            chat_demo(question, o4_state)
            print("Real answer:", answer)
        else:
            user_answer = st.session_state.get("demo_user_answer", None)
            correct = "True" if "True" in answer else "False"
            correct_answer_bool = None
            if user_answer is not None:
                if (user_answer and correct == "True") or (not user_answer and correct == "False"):
                    st.success(f"Correct! The answer is {correct}.")
                    correct_answer_bool = True
                    #st.info(f"Explanation: {answer}")
                else:
                    st.error(f"Incorrect. The correct answer is {correct}.")
                    correct_answer_bool = False
                st.info(f"Explanation exam style: {answer}")
                #st.info(f"Explanation LLM: {st.session_state["explanation_demo"]}")
            file = save_conversation(correct_answer_bool)
            st.download_button(
                label="Download conversations",
                data=file,
                file_name=f"data {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}.pkl",
                #mime="application/pdf"
            )
            if st.button("Reset and Start Again"):
                #time.sleep(5)
                for key in [
                    "selected_question",
                    "question_submitted",
                    "demo_answer_submitted",
                    "init_chat_TF_demo",
                    "messages_demo",
                    "chat_demo",
                    #"explanation_demo"
                ]:
                    if key in st.session_state:
                        del st.session_state[key]
                st.rerun()
            #st.markdown("---")



        

page_names_to_funcs = {
    "User testing": demo,
    "Main menu": intro,
    #"Linear Algebra chat": chat_TF_generation,
    #"Linear Algebra chat": chat,
    #"Practise T/F questions": practice_true_false_questions,
}

demo_name = st.sidebar.selectbox("Select a practice mode:", page_names_to_funcs.keys())
page_names_to_funcs[demo_name]()